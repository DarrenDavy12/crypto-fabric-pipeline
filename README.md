# Crypto Market Data Engineering Pipeline (Microsoft Fabric)

## ðŸ“Œ Project Overview

This project is an end-to-end **data engineering pipeline** that ingests cryptocurrency market data, processes it using a **Lakehouse architecture**, and exposes it for analytics through **Power BI**.

The goal of this project is to demonstrate **core data engineering skills** (ingestion, transformation, validation, storage, and analytics enablement) using modern Microsoft tools, rather than advanced financial analysis.

---


## ðŸ§  Design Decisions & Tradeoffs

* **Fabric Lakehouse** chosen to demonstrate modern Microsoft analytics stack
* **CSV snapshots** used instead of streaming for simplicity and reproducibility
* **Minimal DAX** to keep focus on data engineering rather than analytics
* **No orchestration tool** in this project (handled in a separate ADF project)


## ðŸ—ï¸ Architecture

**High-level flow:**

1. **Extract** â€“ Python script pulls Top 100 cryptocurrency market data (prices, market cap, volume, etc.) from a public API.
2. **Bronze Layer** â€“ Raw CSV files stored in Fabric Lakehouse (immutable snapshots).
3. **Silver Layer** â€“ Cleaned and validated Delta table (schema enforced, basic quality checks).
4. **Semantic Layer** â€“ Fabric Lakehouse table exposed to Power BI.
5. **Visualization** â€“ Power BI dashboard for exploration and monitoring.

```
API / CSV
   â†“
Bronze (Raw CSV)
   â†“
Silver (Clean Delta Table)
   â†“
Power BI Semantic Model
   â†“
Dashboard
```

---

## ðŸ› ï¸ Tech Stack

* **Python** â€“ data extraction and transformation
* **pandas** â€“ data cleaning and validation
* **Microsoft Fabric** â€“ Lakehouse, Delta tables, SQL endpoint
* **OneLake** â€“ unified storage
* **Power BI** â€“ semantic model and dashboard
* **Git & GitHub** â€“ version control

---

## ðŸ“‚ Repository Structure

```
crypto-fabric-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Bronze layer CSV snapshots
â”‚   â””â”€â”€ clean/            # Cleaned CSV outputs (Silver input)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py        # Pulls crypto data
â”‚   â””â”€â”€ transform.py     # Cleans & validates data
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ dashboard.png    # Dashboard screenshot
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ðŸ”„ Data Pipeline Steps

### 1ï¸âƒ£ Extract

* Fetches Top 100 cryptocurrencies
* Stores timestamped raw CSV files
* Ensures reproducibility and auditability

### 2ï¸âƒ£ Transform (Silver Layer)

* Removes invalid or missing values
* Normalizes column names
* Enforces schema consistency
* Outputs clean dataset for analytics

### 3ï¸âƒ£ Load (Fabric Lakehouse)

* Clean data loaded into Fabric Lakehouse
* Stored as Delta tables
* Queryable via SQL Endpoint

### 4ï¸âƒ£ Analytics & Visualization

* Lakehouse table connected to Power BI via OneLake
* No local dashboards or custom APIs required
* Visuals summarize trends and market movements

---

## ðŸ“Š Power BI Dashboard

The dashboard focuses on **operational visibility**, not financial prediction:

* Top 10 cryptocurrencies by market cap
* Daily price change (% gainers / losers)
* Market dominance overview
* Price trends over time
* Volume comparison table
* Interactive filters (date, coin)

> The visuals intentionally remain simple to highlight data reliability and pipeline correctness.

---

## âœ… Data Quality Checks

Basic checks applied during transformation:

* No negative prices
* Required fields not null
* Duplicate rows removed
* Numeric columns validated

Failures are logged during processing.

---

## ðŸš€ How to Run

1. Create a virtual environment
2. Install dependencies
3. Run extraction script
4. Run transformation script
5. Upload cleaned data to Fabric Lakehouse
6. Refresh Power BI semantic model
